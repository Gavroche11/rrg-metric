{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_decimal(x: float, digits: int = 3) -> str:\n",
    "    return f'{x:.{digits}f}'[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_DIR = \"/data1/workspace/bih1122/test_metrics\"\n",
    "\n",
    "def get_metric(model: str,\n",
    "               dataset: str,\n",
    "               metric: Literal[\"bleu\", \"rouge\", \"meteor\", \"bertscore\", \"f1radgraph\",\n",
    "                               \"f1chexbert_macro_f1_14\", \"f1chexbert_micro_f1_14\",\n",
    "                               \"f1chexbert_macro_f1_5\", \"f1chexbert_micro_f1_5\",\n",
    "                               \"sembscore\", \"green\", \"ratescore\"],\n",
    "               metrics_dir: str = METRICS_DIR) -> float:\n",
    "    \n",
    "    metrics = json.load(open(os.path.join(metrics_dir, f\"{model}/{dataset}_test_outputs_metrics.json\")))\n",
    "    green_df = pd.read_csv(os.path.join(metrics_dir, f\"{model}/{dataset}_green_scores.csv\"))\n",
    "\n",
    "    if metric in [\"bleu\", \"rouge\", \"meteor\", \"bertscore\", \"f1radgraph\"]:\n",
    "        return metrics[metric]\n",
    "    elif metric in [\"f1chexbert_macro_f1_14\", \"f1chexbert_micro_f1_14\",\n",
    "                    \"f1chexbert_macro_f1_5\", \"f1chexbert_micro_f1_5\"]:\n",
    "        return metrics[\"chexbert\"][metric.replace(\"f1chexbert_\", \"\")]\n",
    "    elif metric == \"sembscore\":\n",
    "        return metrics[\"chexbert\"][\"sembscore\"]\n",
    "    elif metric == \"green\":\n",
    "        return green_df[\"green_score\"].mean()\n",
    "    elif metric == \"ratescore\":\n",
    "        raise NotImplementedError(\"Rate score is not implemented yet.\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown metric: {metric}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USING_METRICS = [\"bleu\", \"f1chexbert_macro_f1_14\", \"f1chexbert_micro_f1_14\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2.9-full\n",
      "\t& .058 & .232 & .221 & .868 & \\color{red} .402 & \\color{red} .536 & .504 & .574 & .145\n",
      "\t& .108 & .333 & .272 & .891 & \\color{red} .323 & \\color{red} .667 & .312 & .354 & .224 \\\\\n"
     ]
    }
   ],
   "source": [
    "model = 'v2.9-full'\n",
    "\n",
    "metrics = json.load(open(f'/data1/workspace/bih1122/test_metrics/{model}/mimic_test_outputs_metrics.json'))\n",
    "\n",
    "color = '\\color{red}'\n",
    "\n",
    "bleu = foo(metrics['bleu'])\n",
    "meteor = foo(metrics['meteor'])\n",
    "rouge = foo(metrics['rouge'])\n",
    "bertscore = foo(metrics['bertscore'])\n",
    "macro14 = foo(metrics['f1chexbert']['macro_f1_14'])\n",
    "micro14 = foo(metrics['f1chexbert']['micro_f1_14'])\n",
    "macro5 = foo(metrics['f1chexbert']['macro_f1_5'])\n",
    "micro5 = foo(metrics['f1chexbert']['micro_f1_5'])\n",
    "micro14 = foo(metrics['f1chexbert']['micro_f1_14'])\n",
    "f1rg = foo(metrics['f1radgraph'])\n",
    "\n",
    "mimic = f\"& {bleu} & {meteor} & {rouge} & {bertscore} & {color} {macro14} & {color} {micro14} & {macro5} & {micro5} & {f1rg}\"\n",
    "\n",
    "metrics = json.load(open(f'/data1/workspace/bih1122/test_metrics/{model}/openi_test_outputs_metrics.json'))\n",
    "\n",
    "bleu = foo(metrics['bleu'])\n",
    "meteor = foo(metrics['meteor'])\n",
    "rouge = foo(metrics['rouge'])\n",
    "bertscore = foo(metrics['bertscore'])\n",
    "macro14 = foo(metrics['f1chexbert']['macro_f1_14'])\n",
    "micro14 = foo(metrics['f1chexbert']['micro_f1_14'])\n",
    "macro5 = foo(metrics['f1chexbert']['macro_f1_5'])\n",
    "micro5 = foo(metrics['f1chexbert']['micro_f1_5'])\n",
    "micro14 = foo(metrics['f1chexbert']['micro_f1_14'])\n",
    "f1rg = foo(metrics['f1radgraph'])\n",
    "\n",
    "openi = f\"& {bleu} & {meteor} & {rouge} & {bertscore} & {color} {macro14} & {color} {micro14} & {macro5} & {micro5} & {f1rg}\"\n",
    "\n",
    "print(f\"{model}\\n\\t{mimic}\\n\\t{openi} \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rrg-metric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
